{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "making environment for dqn 2 nd time.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girish342/flaskSaaS/blob/master/making_environment_for_dqn_2_nd_time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3P7GSaqRXgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np  # for array stuff and random\n",
        "from PIL import Image  # for creating visual of our env\n",
        "import cv2  # for showing our visual live\n",
        "import matplotlib.pyplot as plt  # for graphing our mean rewards over time\n",
        "import pickle  # to save/load Q-Tables\n",
        "from matplotlib import style  # to make pretty charts because it matters.\n",
        "import time  # using this to keep track of our saved Q-Tables.\n",
        "\n",
        "style.use(\"ggplot\")  # setting our style!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byk1D4foRtyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we r gonna create our own environment where square grid will be 10 X 10 size because it will take only 15 mb of meomory if we use 20 X 20 then it wil take 195 mb\n",
        "# we r gonna define hyperparameters here befor defining our environment\n",
        "\n",
        "SIZE = 10\n",
        "\n",
        "HM_EPISODES = 25000\n",
        "MOVE_PENALTY = 1  # feel free to tinker with these,these r random values by sentdex\n",
        "ENEMY_PENALTY = 300  # feel free to tinker with these!\n",
        "FOOD_REWARD = 25  # feel free to tinker with these!\n",
        "epsilon = 0.5  # randomness\n",
        "EPS_DECAY = 0.9999  # Every episode will be epsilon*EPS_DECAY,it converts learnig into earning \n",
        "SHOW_EVERY = 1000  # how often to play through env visually, weather we r going in right direction or not \n",
        "\n",
        "\n",
        "start_q_table = None # weather we have q table or not, there is no need to have q table if we dont have it\n",
        "\n",
        "LEARNING_RATE = 0.1 # what step to be taken to achieve GRADIENT DESCENT\n",
        "DISCOUNT = 0.95   # its tells what is future vs current reward\n",
        "\n",
        "\n",
        "# we r giving numbers to the players\n",
        "PLAYER_N = 1  # player key in dict\n",
        "FOOD_N = 2  # food key in dict\n",
        "ENEMY_N = 3  # enemy key in dict\n",
        "\n",
        "# we r using dictinory for colors why we using dictionary(because it gives colors to respected player,food,enenmy) ,it contains bgr = blue,green,red.and giving numbers to colors \n",
        "d = {1: (255, 175, 0),  # blueish color(1 = blue,2 = green)\n",
        "     2: (0, 255, 0),  # green\n",
        "     3: (0, 0, 255)}  # red\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxJUU-uVXqWx",
        "colab_type": "code",
        "outputId": "a50fb75f-bfa4-49cf-fe11-89ebd6dc0110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# all hyperparmaters, numbers, colors,dependencies are defined\n",
        "# we will now create environment by specifying class which is oop \n",
        "#class blob, creating init method so that we can interact with objects with class.\n",
        "# i have to get intuition which class to use , how to use, when to use and happy learning and changing world for bertterment of humanity.\n",
        "# string class to be used in oop to debug the process which i dont know which return the function along with x and y.\n",
        "# __init__ constructs class, it is gateway for class to interact with its own attributes like property, tax/gov, own money\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Blob:\n",
        "  def __init__(self):\n",
        "    self.x = np.random.randint(0,SIZE)  # blob is square,when we start the environment it will start randomly as random.rand.\n",
        "    self.y = np.random.randint(0,SIZE)  # x = input, y = output ;input dim = o to size which is 10 x 10\n",
        "    \n",
        "  def __string__(self):                 # we are using string because to debug according to sentdex, which i dont get it.\n",
        "    return f\"{self.x}, {self.y}\"        # what to debug and how to debug i dont know just according to sentdex i have to research it\n",
        "  \n",
        "  \n",
        "  def __sub__(self,other):            # we r crating sub(vajabaki) method so that we can substract from one blob to another blob        \n",
        "    return(self.x-other.x, self.y-other.y) # first sub = player - food;2nd sub = player - enemy; it is called as opertor overloading which i dont know; \n",
        "\n",
        "  \n",
        "# we r crating random action where the agent will move diagonally,up,low. we can add the actions if we want to add it \n",
        "# but magical thing about q network is that WE R NOT SPECIFYING THE ACTIONS IT WILL FIGURE OUT ON ITS OWN THATS POWER OF Q LEARINGING\n",
        "# this is your typical function no need to use __init__ method\n",
        "# without permision of self, class cannot interact with its own attributes. for eg courts give permission to father to met his kids.\n",
        "# these r 4 actions (up,down,diagonal right,left)\n",
        "\n",
        "  def action(self,choice):  \n",
        "    \n",
        "    if choice == 0:\n",
        "      self.move(x=1,y=1)\n",
        "      \n",
        "    if choice == 1:\n",
        "      self.move(x=-1,y=-1)\n",
        "      \n",
        "      \n",
        "    if choice == 2:\n",
        "      self.move(x=-1,y=1)\n",
        "      \n",
        "    if choice == 3:\n",
        "      self.move(x=1,y=-1)\n",
        "      \n",
        "      \n",
        "# self is hidden parameter in other language but in python we have to give respect to self in special way to felicitate it specially\n",
        "# self rationlize parameters which we r passing to the class otherwisse class should have raised an error if we directly pass to it\n",
        "# in move method we r moving x and y randomly by putting them false in move def intially so that we can explore the environment.\n",
        "# python its boolian (if not x means  random ) its x or random \n",
        "# we r moving randomly, if we r not getting values for x and y. these are possibilites that s why we have not used __init__which is fix method.\n",
        "# in __init__ method everthing is fixed.it canot be used in random method\n",
        "\n",
        "def move(self, x=False, y=False):  # we have to move x and y randomly so that we r making them false we can do any thing here so that  its called as oop\n",
        "    if not x:\n",
        "      self.x +=  np.random.randint(-1, 2)  # why it is (-1, 2) i didnot get it.\n",
        "      \n",
        "    else:\n",
        "      self.x += x  # if it doesnot move randomly move in desired direction.\n",
        "      \n",
        "# its python bullian y if not y = random.\n",
        "\n",
        "    if not y:\n",
        "      self.y +=  np.random.randint(-1, 2)\n",
        "      \n",
        "    else:\n",
        "      self.y += y\n",
        "      \n",
        "# x cannot less than 0 if it is less than 0 then make it 0(first 2 lines).its oop we can do anything here\n",
        "# x should not be more than size if it so then make it of size,size-1= 0 to 9 we r giving bounries to x.\n",
        "    if self.x < 0:\n",
        "      self.x = 0\n",
        "    if self.x > SIZE-1:\n",
        "      self.x = SIZE-1 \n",
        "      \n",
        "# same about y it shouldnot go oversize and undersize(it shouldnot cross boundries)    \n",
        "    if self.y < 0:\n",
        "      self.y= 0\n",
        "    if self.y > SIZE-1:\n",
        "      self.y = SIZE-1 \n",
        "      \n",
        "      \n",
        "# we r checking all parameters here our class object construction is done \n",
        "# write now we will make q table\n",
        "# subject understanding is 80%                                           26 june 2019\n",
        "\n",
        "player = Blob()\n",
        "food = Blob()\n",
        "enemy = Blob()\n",
        "\n",
        "\n",
        "print(player)\n",
        "print(food)\n",
        "print(player-food)\n",
        "player.move()\n",
        "print(player-food)\n",
        "player.action(2)\n",
        "print(player-food)     \n",
        "\n",
        "\n",
        "# upto now we have created dependancies,defined hyperparameters,createdd environment(action,movement,limitations,defining player.food.enemy.\n",
        "\n",
        "\n",
        "if start_q_table is None:                 # if we dont have the q table we have to create one\n",
        "  q_table = {}                            # it is dictionary because it provides relation bet state and action thats why it is dictionary\n",
        "  for i in range(-SIZE+1, SIZE):           # we are creating q table with range of sizes,from negative to positive max upto 9\n",
        "    for ii in range(-SIZE+1, SIZE):        # first i = player,2nd i = food, 3rd i = enenmy, 4th i = food-player\n",
        "      for iii in range(-SIZE+1, SIZE):\n",
        "        for iiii in range(-SIZE+1, SIZE): # adding all above values to q table which is list of tuple of tuples\n",
        "          q_table[((i,ii),(iii,iiii))] = [np.random.uniform(-5,0) for i in range (4)] # we have 4 actions so observation space should 4 so its includes bet 0 to -5, but why these values i dont know\n",
        "          \n",
        "else:\n",
        "  with open(start_q_table,\"rb\")as f: # if we dont have q table then we save it with pickle and load it q_tablel.\n",
        "    q_table = pickle.load(f)\n",
        "    \n",
        "    \n",
        "# upto now we have crated environment,q table and aslo loaded the q table now we start iteration through episodes.\n",
        "\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in range(HM_EPISODES):  # we r checking episode reward by reintializing player,food,enemy\n",
        "  player = Blob()\n",
        "  food = Blob()\n",
        "  enemy = Blob()\n",
        "  \n",
        "  \n",
        "  if episode % SHOW_EVERY == 0:\n",
        "    print(f\"on #{episode}, epsilon is {epsilon}\")  # printing every episode and epsilon to see where we r\n",
        "    print(f\"{SHOW_EVERY} ep mean: {np.mean(episode_rewards[-SHOW_EVERY:])}\") # we r gonna display min episode reward\n",
        "    \n",
        "    show = True\n",
        "    \n",
        "  else:\n",
        "    show = False\n",
        "    \n",
        "    \n",
        "    episode_reward = 0  # from starting of state episode reward will be 0.\n",
        "    \n",
        "    for i in range(200):  # we r taking 200 steps in one episode thats we wrote episode_reward not episode_rewards.\n",
        "      obs = (player-food,player-enemy) # we take this observation from sub method which is operator overloding which is i dont know\n",
        "      #rint(obs)\n",
        "      \n",
        "      if np.random.random() > epsilon:  # if exploitation is > exploration\n",
        "        action = np.argmax(q_table[obs]) # get max action \n",
        "        \n",
        "      else:\n",
        "        action = np.random.randint(0,4)  # either we get max action or predefined 4 actions\n",
        "        \n",
        "      player.action(action)\n",
        "      \n",
        "      #enemy.move  # should not move intially because it complecates the thing acc to sentdex\n",
        "      #food.move\n",
        "      \n",
        "# after taking actions we will decide what is reward whether its penulty or reward defing parameters for penulty.      \n",
        "      if player.x == enemy.x and player.y == enemy.y:\n",
        "        reward = -ENEMY_PENULTY   # its negative otherwise player will love the enemy if it is not punished,we have to avoid enemy\n",
        "      \n",
        "        \n",
        "      elif player.x == food.x and player.y == food.y:\n",
        "        reward = FOOD_REWARD\n",
        "        \n",
        "      else:\n",
        "        reward = -MOVE_PENULTY  # every failure is punished thats life we have to be succeessful otherwise pay the penulty.\n",
        "        \n",
        "        \n",
        "      new_obs = (player-food, player-enemy)\n",
        "      \n",
        "      max_future_q = np.max(q_table(new_obs))\n",
        "      current_q = q_table[obs][action]\n",
        "      \n",
        "      if reward == FOOD_REWARD:\n",
        "        new_q = FOOD_REWARD\n",
        "        \n",
        "        \n",
        "# max future_q is calculated on current_q value because we want future_q value max(it is bet 0 and 1).\n",
        "# discount = how much we care about future rewards rather than immeditate rewards we need both rewards because immediate reward = sustain, future_reward = to grow\n",
        "# new_q = (1-alpha) * current_q + alpha * learned_value(reward+ discount*max_future_q)\n",
        "# we use learining rate so much because its decides gradient descent which backpropagetes and modify weights so that we have minimum error and maximum result.\n",
        "# this equation applies to life because how we play with our life how we balance it its completely here it decides our present condition,next line\n",
        "# this equation describes current situation(penulties,rewards) and learning future rewards future its brutal work hard my boy.\n",
        "\n",
        "      else:\n",
        "    \n",
        "        new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q) # its optimala q value for optimal state and optimal action, it is based on learned value on preivious episodes current q value..\n",
        "        \n",
        "      new_q = q_table[obs][action]\n",
        " \n",
        "# right now q learing is completed we have to see what we have done and graph some matrix\n",
        "\n",
        "      if show:\n",
        "        env = np.zeros((SIZE, SIZE, 3), dtype=np.uint8)  # i m wrapping img with int 8,converting from bgr to rgb.\n",
        "        env[food.x][food.y] = d[FOOD_N]\n",
        "        env[player.x][player.y] = d[PLAYER_N]\n",
        "        env[enemy.x][enemy.y] = d[ENEMY_N]\n",
        "        img = Image.fromarray[env,'RGB']   # image in bgr shape we r converting into rgb\n",
        "        img = img.resize((300,300))  # size of imge is 10 x 10 converting size into 300 because we cant see properly\n",
        "        cv2.imshow = ((\"image\",np.array(img)))\n",
        "        if reward == FOOD_REWARD or reward == -ENEMY_PENULTY:  # if player got food or hit enemy we will pause\n",
        "          if cv2.waitKey(500) & 0xFF == ord('q'):             # 500 miliseconds pause and break with q key\n",
        "            break\n",
        "        \n",
        "        else:\n",
        "          if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "            \n",
        "      episode_reward += reward    # if this happens we end episode and start new episode\n",
        "      if reward == FOOD_REWARD or reward == -ENEMY_PENALTY:\n",
        "        break\n",
        "        \n",
        "        \n",
        "    episode_rewards.append(episode_reward)\n",
        "    epsilon *= EPS_DECAY\n",
        "      \n",
        "\n",
        "moving_avg = np.convolve(episode_rewards,np.ones((SHOW_EVERY,)) / SHOW_EVERY, mode='valid')\n",
        "plt.plot([i for i in range(len(moving_avg))],moving_avg)\n",
        "plt.ylabel(f\"Reward{SHOW_EVERY}ma\")\n",
        "plt.xlabel(\"episode #\")\n",
        "plt.plot()\n",
        "\n",
        "\n",
        "with open(f\"q_table-{int(time.time())}.pickle\",\"wb\") as f:\n",
        "  pickle.dump(q_table,f)\n",
        "            \n",
        "          \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "        \n",
        "        \n",
        "      \n",
        "        \n",
        "        \n",
        "      \n",
        "      \n",
        "    \n",
        "    \n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        " \n",
        "    \n",
        "   \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "      \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "     \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        " \n",
        "  \n",
        "    \n",
        " \n",
        " \n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.Blob object at 0x7f2276a185f8>\n",
            "<__main__.Blob object at 0x7f2276a18630>\n",
            "(-4, -2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-db7d032130df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Blob' object has no attribute 'move'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgc0MFNKAeJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}